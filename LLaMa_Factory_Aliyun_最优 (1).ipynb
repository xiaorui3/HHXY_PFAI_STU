{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a60c63-090b-473e-a0df-dfc89c8ba544",
   "metadata": {},
   "source": [
    "1. Install LLaMa-Factory 安装LLaMa-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e92016-d96e-4742-b229-e37fd11bdad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "error: RPC failed; curl 16 Error in the HTTP2 framing layer\n",
      "fatal: expected flush after ref listing\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9381cf-31c0-4dd9-94e3-6d5b34de7ed8",
   "metadata": {},
   "source": [
    "2. Configure Environment 配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acac4706-5134-4172-be2d-932648d90756",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xiaorui/LLaMA-Factory\n",
      "/bin/bash: line 1: pip: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: pip: command not found\n",
      "/bin/bash: line 1: pip: command not found\n",
      "/bin/bash: line 1: pip: command not found\n",
      "/bin/bash: line 1: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory\n",
    "pip install -r requirements.txt\n",
    "pip install transformers_stream_generator bitsandbytes tiktoken auto-gptq optimum autoawq\n",
    "pip install --upgrade tensorflow\n",
    "pip uninstall flash-attn -y\n",
    "pip install vllm==0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bd8d6",
   "metadata": {},
   "source": [
    "(Optional) Generate public network link （可选）生成公网链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd355b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/share=False/share=True/' src/train_web.py\n",
    "!sed -i 's/share=False/share=True/' src/web_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d085f-1e63-4ed7-ad45-d806f08499fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -e .[metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99385d22-d597-4a3b-a3fa-e4cfbfcfda36",
   "metadata": {},
   "source": [
    "3.Run LLaMa-Factory 运行LLaMa-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e5420-f5b6-4f6c-bb6f-3b0946ad2983",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-11-28T04:58:10.173679Z",
     "iopub.status.busy": "2024-11-28T04:58:10.173163Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/LLaMA-Factory\n",
      "2024-11-28 12:58:14.482787: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 12:58:14.484662: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-28 12:58:14.521392: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-28 12:58:14.521731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 12:58:15.266494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-11-28 12:58:21,942] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "2024-11-28 12:59:02.976246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-11-28 12:59:05,230] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO|2024-11-28 12:59:07] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 12:59:07,834 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 12:59:07,835 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:07,836 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:07,837 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:07,837 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:07,837 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:07,837 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:07,837 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-28 12:59:08,076 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 12:59:08,077 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 12:59:08,078 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:08,079 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:08,079 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:08,079 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:08,079 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:08,079 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 12:59:08,079 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-28 12:59:08,314 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2024-11-28 12:59:08] llamafactory.data.loader:157 >> Loading dataset alpaca_zh_demo.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 1000 examples [00:00, 41709.47 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 1000/1000 [00:00<00:00, 5605\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:02<00:00, 419.\n",
      "training example:\n",
      "input_ids:\n",
      "[33975, 25, 220, 102450, 62926, 104136, 89012, 22382, 44177, 101047, 100369, 99891, 101911, 5122, 102150, 101911, 33108, 8903, 63109, 36587, 8997, 71703, 25, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151643]\n",
      "inputs:\n",
      "Human: 识别并解释给定列表中的两个科学理论：细胞理论和日心说。\n",
      "Assistant:细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
      "\n",
      "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 102150, 101911, 20412, 100206, 99891, 104111, 101911, 3837, 99652, 100140, 55338, 100702, 31914, 100132, 67071, 48934, 30709, 105166, 106251, 8545, 102150, 31838, 104384, 1773, 100346, 114651, 104111, 99896, 101911, 3837, 100140, 102150, 20412, 55338, 100206, 105166, 100166, 33108, 98380, 75317, 3837, 104152, 100206, 100132, 67071, 46944, 57191, 101213, 102150, 101286, 3837, 102150, 101097, 67338, 102150, 110935, 100394, 100676, 102150, 1773, 100147, 101911, 67071, 106929, 22382, 120806, 5373, 99330, 101190, 31843, 33108, 100167, 100809, 34204, 16, 23, 18, 24, 7948, 104181, 101080, 3407, 8903, 63109, 36587, 104442, 101281, 20412, 101281, 38176, 9370, 99488, 3837, 105884, 3837, 113837, 102074, 101281, 108215, 9370, 101911, 1773, 99487, 101911, 112479, 105062, 29490, 63109, 36587, 101313, 3837, 100140, 102493, 102095, 105339, 9370, 99488, 1773, 8903, 63109, 36587, 9370, 101080, 28946, 20412, 99685, 99470, 72225, 13935, 99826, 99243, 99685, 3837, 104677, 16, 21, 101186, 84607, 102098, 108124, 101712, 26940, 35727, 31914, 104001, 67831, 87243, 109268, 34187, 101281, 38176, 113837, 102074, 101281, 104001, 9370, 104949, 3837, 17714, 35727, 104179, 103949, 107759, 102334, 102007, 1773, 151643]\n",
      "labels:\n",
      "细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
      "\n",
      "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|endoftext|>\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 12:59:12,571 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 12:59:12,572 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3934] 2024-11-28 12:59:12,597 >> loading weights file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1670] 2024-11-28 12:59:12,597 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2024-11-28 12:59:12,599 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:24<00:00, 21.19s/it]\n",
      "[INFO|modeling_utils.py:4800] 2024-11-28 13:00:37,421 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-11-28 13:00:37,421 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1049] 2024-11-28 13:00:37,424 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-11-28 13:00:37,424 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "[INFO|2024-11-28 13:00:37] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2024-11-28 13:00:37] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-11-28 13:00:37] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2024-11-28 13:00:37] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2024-11-28 13:00:37] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,up_proj,v_proj,o_proj,gate_proj,k_proj,down_proj\n",
      "[INFO|2024-11-28 13:00:37] llamafactory.model.loader:157 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n",
      "/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:698] 2024-11-28 13:00:38,014 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2313] 2024-11-28 13:00:38,313 >> ***** Running training *****\n",
      "[INFO|trainer.py:2314] 2024-11-28 13:00:38,313 >>   Num examples = 1,000\n",
      "[INFO|trainer.py:2315] 2024-11-28 13:00:38,313 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2316] 2024-11-28 13:00:38,313 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2319] 2024-11-28 13:00:38,313 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2320] 2024-11-28 13:00:38,313 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2321] 2024-11-28 13:00:38,313 >>   Total optimization steps = 186\n",
      "[INFO|trainer.py:2322] 2024-11-28 13:00:38,317 >>   Number of trainable parameters = 20,185,088\n",
      "  3%|█▏                                         | 5/186 [00:18<10:58,  3.64s/it][INFO|2024-11-28 13:00:57] llamafactory.train.callbacks:157 >> {'loss': 1.5324, 'learning_rate': 4.9911e-05, 'epoch': 0.08}\n",
      "{'loss': 1.5324, 'grad_norm': 0.5554704070091248, 'learning_rate': 4.9910902453260824e-05, 'epoch': 0.08}\n",
      "  5%|██▎                                       | 10/186 [00:36<10:47,  3.68s/it][INFO|2024-11-28 13:01:15] llamafactory.train.callbacks:157 >> {'loss': 1.4853, 'learning_rate': 4.9644e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4853, 'grad_norm': 0.2192809283733368, 'learning_rate': 4.964424488287009e-05, 'epoch': 0.16}\n",
      "  8%|███▍                                      | 15/186 [00:54<10:27,  3.67s/it][INFO|2024-11-28 13:01:32] llamafactory.train.callbacks:157 >> {'loss': 1.5536, 'learning_rate': 4.9202e-05, 'epoch': 0.24}\n",
      "{'loss': 1.5536, 'grad_norm': 0.27538374066352844, 'learning_rate': 4.920192797165511e-05, 'epoch': 0.24}\n",
      " 11%|████▌                                     | 20/186 [01:14<10:50,  3.92s/it][INFO|2024-11-28 13:01:52] llamafactory.train.callbacks:157 >> {'loss': 1.3500, 'learning_rate': 4.8587e-05, 'epoch': 0.32}\n",
      "{'loss': 1.35, 'grad_norm': 0.36620578169822693, 'learning_rate': 4.858710446774951e-05, 'epoch': 0.32}\n",
      " 13%|█████▋                                    | 25/186 [01:31<09:04,  3.38s/it][INFO|2024-11-28 13:02:09] llamafactory.train.callbacks:157 >> {'loss': 1.3213, 'learning_rate': 4.7804e-05, 'epoch': 0.40}\n",
      "{'loss': 1.3213, 'grad_norm': 0.409252405166626, 'learning_rate': 4.780415671242334e-05, 'epoch': 0.4}\n",
      " 16%|██████▊                                   | 30/186 [01:51<10:29,  4.03s/it][INFO|2024-11-28 13:02:29] llamafactory.train.callbacks:157 >> {'loss': 1.4036, 'learning_rate': 4.6859e-05, 'epoch': 0.48}\n",
      "{'loss': 1.4036, 'grad_norm': 0.32931143045425415, 'learning_rate': 4.685866540361456e-05, 'epoch': 0.48}\n",
      " 19%|███████▉                                  | 35/186 [02:10<09:38,  3.83s/it][INFO|2024-11-28 13:02:48] llamafactory.train.callbacks:157 >> {'loss': 1.4079, 'learning_rate': 4.5757e-05, 'epoch': 0.56}\n",
      "{'loss': 1.4079, 'grad_norm': 0.3801755905151367, 'learning_rate': 4.5757369817809415e-05, 'epoch': 0.56}\n",
      " 22%|█████████                                 | 40/186 [02:28<08:48,  3.62s/it][INFO|2024-11-28 13:03:06] llamafactory.train.callbacks:157 >> {'loss': 1.4215, 'learning_rate': 4.4508e-05, 'epoch': 0.64}\n",
      "{'loss': 1.4215, 'grad_norm': 0.5079466700553894, 'learning_rate': 4.45081197738023e-05, 'epoch': 0.64}\n",
      " 24%|██████████▏                               | 45/186 [02:48<09:01,  3.84s/it][INFO|2024-11-28 13:03:26] llamafactory.train.callbacks:157 >> {'loss': 1.3243, 'learning_rate': 4.3120e-05, 'epoch': 0.72}\n",
      "{'loss': 1.3243, 'grad_norm': 0.4514698088169098, 'learning_rate': 4.3119819680728e-05, 'epoch': 0.72}\n",
      " 27%|███████████▎                              | 50/186 [03:05<07:45,  3.43s/it][INFO|2024-11-28 13:03:43] llamafactory.train.callbacks:157 >> {'loss': 1.4545, 'learning_rate': 4.1602e-05, 'epoch': 0.80}\n",
      "{'loss': 1.4545, 'grad_norm': 0.4827057719230652, 'learning_rate': 4.160236506918098e-05, 'epoch': 0.8}\n",
      " 30%|████████████▍                             | 55/186 [03:23<07:50,  3.59s/it][INFO|2024-11-28 13:04:01] llamafactory.train.callbacks:157 >> {'loss': 1.4400, 'learning_rate': 3.9967e-05, 'epoch': 0.88}\n",
      "{'loss': 1.44, 'grad_norm': 0.43589991331100464, 'learning_rate': 3.9966572057815373e-05, 'epoch': 0.88}\n",
      " 32%|█████████████▌                            | 60/186 [03:41<07:40,  3.65s/it][INFO|2024-11-28 13:04:20] llamafactory.train.callbacks:157 >> {'loss': 1.4108, 'learning_rate': 3.8224e-05, 'epoch': 0.96}\n",
      "{'loss': 1.4108, 'grad_norm': 0.41419246792793274, 'learning_rate': 3.822410025817406e-05, 'epoch': 0.96}\n",
      " 35%|██████████████▋                           | 65/186 [04:01<07:44,  3.84s/it][INFO|2024-11-28 13:04:39] llamafactory.train.callbacks:157 >> {'loss': 1.4155, 'learning_rate': 3.6387e-05, 'epoch': 1.04}\n",
      "{'loss': 1.4155, 'grad_norm': 0.4224119186401367, 'learning_rate': 3.638736966726585e-05, 'epoch': 1.04}\n",
      " 38%|███████████████▊                          | 70/186 [04:18<06:50,  3.54s/it][INFO|2024-11-28 13:04:56] llamafactory.train.callbacks:157 >> {'loss': 1.3912, 'learning_rate': 3.4469e-05, 'epoch': 1.12}\n",
      "{'loss': 1.3912, 'grad_norm': 0.4089176058769226, 'learning_rate': 3.44694721402644e-05, 'epoch': 1.12}\n",
      " 40%|████████████████▉                         | 75/186 [04:36<06:41,  3.62s/it][INFO|2024-11-28 13:05:15] llamafactory.train.callbacks:157 >> {'loss': 1.3273, 'learning_rate': 3.2484e-05, 'epoch': 1.20}\n",
      "{'loss': 1.3273, 'grad_norm': 0.435075968503952, 'learning_rate': 3.2484078074333954e-05, 'epoch': 1.2}\n",
      " 43%|██████████████████                        | 80/186 [04:55<06:35,  3.73s/it][INFO|2024-11-28 13:05:34] llamafactory.train.callbacks:157 >> {'loss': 1.3186, 'learning_rate': 3.0445e-05, 'epoch': 1.28}\n",
      "{'loss': 1.3186, 'grad_norm': 0.536579430103302, 'learning_rate': 3.0445338968721287e-05, 'epoch': 1.28}\n",
      " 46%|███████████████████▏                      | 85/186 [05:15<06:29,  3.85s/it][INFO|2024-11-28 13:05:53] llamafactory.train.callbacks:157 >> {'loss': 1.3089, 'learning_rate': 2.8368e-05, 'epoch': 1.36}\n",
      "{'loss': 1.3089, 'grad_norm': 0.5827146768569946, 'learning_rate': 2.836778655564653e-05, 'epoch': 1.36}\n",
      " 48%|████████████████████▎                     | 90/186 [05:35<06:37,  4.14s/it][INFO|2024-11-28 13:06:13] llamafactory.train.callbacks:157 >> {'loss': 1.4623, 'learning_rate': 2.6266e-05, 'epoch': 1.44}\n",
      "{'loss': 1.4623, 'grad_norm': 0.5393600463867188, 'learning_rate': 2.6266229220967818e-05, 'epoch': 1.44}\n",
      " 51%|█████████████████████▍                    | 95/186 [05:53<05:30,  3.63s/it][INFO|2024-11-28 13:06:31] llamafactory.train.callbacks:157 >> {'loss': 1.2704, 'learning_rate': 2.4156e-05, 'epoch': 1.52}\n",
      "{'loss': 1.2704, 'grad_norm': 0.5748312473297119, 'learning_rate': 2.4155646452913296e-05, 'epoch': 1.52}\n",
      " 54%|██████████████████████                   | 100/186 [06:11<05:16,  3.69s/it][INFO|2024-11-28 13:06:50] llamafactory.train.callbacks:157 >> {'loss': 1.4529, 'learning_rate': 2.2051e-05, 'epoch': 1.60}\n",
      "{'loss': 1.4529, 'grad_norm': 0.49093931913375854, 'learning_rate': 2.2051082071228854e-05, 'epoch': 1.6}\n",
      " 54%|██████████████████████                   | 100/186 [06:11<05:16,  3.69s/it][INFO|trainer.py:3801] 2024-11-28 13:06:50,312 >> Saving model checkpoint to saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/checkpoint-100\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 13:06:50,335 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 13:06:50,335 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-28 13:06:50,436 >> tokenizer config file saved in saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-28 13:06:50,436 >> Special tokens file saved in saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/checkpoint-100/special_tokens_map.json\n",
      " 56%|███████████████████████▏                 | 105/186 [06:32<05:14,  3.89s/it][INFO|2024-11-28 13:07:10] llamafactory.train.callbacks:157 >> {'loss': 1.2702, 'learning_rate': 1.9968e-05, 'epoch': 1.68}\n",
      "{'loss': 1.2702, 'grad_norm': 0.6170361638069153, 'learning_rate': 1.9967536997783494e-05, 'epoch': 1.68}\n",
      " 59%|████████████████████████▏                | 110/186 [06:51<04:51,  3.83s/it][INFO|2024-11-28 13:07:29] llamafactory.train.callbacks:157 >> {'loss': 1.3499, 'learning_rate': 1.7920e-05, 'epoch': 1.76}\n",
      "{'loss': 1.3499, 'grad_norm': 0.6282036900520325, 'learning_rate': 1.79198623329424e-05, 'epoch': 1.76}\n",
      " 62%|█████████████████████████▎               | 115/186 [07:10<04:28,  3.78s/it][INFO|2024-11-28 13:07:48] llamafactory.train.callbacks:157 >> {'loss': 1.2833, 'learning_rate': 1.5923e-05, 'epoch': 1.84}\n",
      "{'loss': 1.2833, 'grad_norm': 0.47371965646743774, 'learning_rate': 1.5922653499838137e-05, 'epoch': 1.84}\n",
      " 65%|██████████████████████████▍              | 120/186 [07:28<03:58,  3.61s/it][INFO|2024-11-28 13:08:06] llamafactory.train.callbacks:157 >> {'loss': 1.4773, 'learning_rate': 1.3990e-05, 'epoch': 1.92}\n",
      "{'loss': 1.4773, 'grad_norm': 0.7376434803009033, 'learning_rate': 1.399014621105914e-05, 'epoch': 1.92}\n",
      " 67%|███████████████████████████▌             | 125/186 [07:43<03:09,  3.10s/it][INFO|2024-11-28 13:08:21] llamafactory.train.callbacks:157 >> {'loss': 1.5045, 'learning_rate': 1.2136e-05, 'epoch': 2.00}\n",
      "{'loss': 1.5045, 'grad_norm': 1.4132580757141113, 'learning_rate': 1.2136114999284288e-05, 'epoch': 2.0}\n",
      " 70%|████████████████████████████▋            | 130/186 [08:03<03:34,  3.83s/it][INFO|2024-11-28 13:08:41] llamafactory.train.callbacks:157 >> {'loss': 1.4088, 'learning_rate': 1.0374e-05, 'epoch': 2.08}\n",
      "{'loss': 1.4088, 'grad_norm': 0.5517154335975647, 'learning_rate': 1.0373775035117305e-05, 'epoch': 2.08}\n",
      " 73%|█████████████████████████████▊           | 135/186 [08:22<03:16,  3.86s/it][INFO|2024-11-28 13:09:01] llamafactory.train.callbacks:157 >> {'loss': 1.3350, 'learning_rate': 8.7157e-06, 'epoch': 2.16}\n",
      "{'loss': 1.335, 'grad_norm': 0.6375329494476318, 'learning_rate': 8.715687931944449e-06, 'epoch': 2.16}\n",
      " 75%|██████████████████████████████▊          | 140/186 [08:41<02:44,  3.57s/it][INFO|2024-11-28 13:09:19] llamafactory.train.callbacks:157 >> {'loss': 1.3035, 'learning_rate': 7.1737e-06, 'epoch': 2.24}\n",
      "{'loss': 1.3035, 'grad_norm': 0.7251455187797546, 'learning_rate': 7.173672209219495e-06, 'epoch': 2.24}\n",
      " 78%|███████████████████████████████▉         | 145/186 [08:59<02:26,  3.57s/it][INFO|2024-11-28 13:09:38] llamafactory.train.callbacks:157 >> {'loss': 1.3071, 'learning_rate': 5.7587e-06, 'epoch': 2.32}\n",
      "{'loss': 1.3071, 'grad_norm': 0.7265884280204773, 'learning_rate': 5.758719052376693e-06, 'epoch': 2.32}\n",
      " 81%|█████████████████████████████████        | 150/186 [09:17<02:09,  3.60s/it][INFO|2024-11-28 13:09:56] llamafactory.train.callbacks:157 >> {'loss': 1.3497, 'learning_rate': 4.4809e-06, 'epoch': 2.40}\n",
      "{'loss': 1.3497, 'grad_norm': 0.6734094619750977, 'learning_rate': 4.480913969818098e-06, 'epoch': 2.4}\n",
      " 83%|██████████████████████████████████▏      | 155/186 [09:37<01:59,  3.84s/it][INFO|2024-11-28 13:10:15] llamafactory.train.callbacks:157 >> {'loss': 1.2915, 'learning_rate': 3.3494e-06, 'epoch': 2.48}\n",
      "{'loss': 1.2915, 'grad_norm': 0.6323840022087097, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.48}\n",
      " 86%|███████████████████████████████████▎     | 160/186 [09:54<01:33,  3.58s/it][INFO|2024-11-28 13:10:32] llamafactory.train.callbacks:157 >> {'loss': 1.2769, 'learning_rate': 2.3721e-06, 'epoch': 2.56}\n",
      "{'loss': 1.2769, 'grad_norm': 0.5955253839492798, 'learning_rate': 2.372137318741968e-06, 'epoch': 2.56}\n",
      " 89%|████████████████████████████████████▎    | 165/186 [10:13<01:16,  3.66s/it][INFO|2024-11-28 13:10:52] llamafactory.train.callbacks:157 >> {'loss': 1.1668, 'learning_rate': 1.5562e-06, 'epoch': 2.64}\n",
      "{'loss': 1.1668, 'grad_norm': 0.735433042049408, 'learning_rate': 1.5561966963229924e-06, 'epoch': 2.64}\n",
      " 91%|█████████████████████████████████████▍   | 170/186 [10:31<00:57,  3.61s/it][INFO|2024-11-28 13:11:10] llamafactory.train.callbacks:157 >> {'loss': 1.4275, 'learning_rate': 9.0736e-07, 'epoch': 2.72}\n",
      "{'loss': 1.4275, 'grad_norm': 0.7087110280990601, 'learning_rate': 9.073589027514789e-07, 'epoch': 2.72}\n",
      " 94%|██████████████████████████████████████▌  | 175/186 [10:51<00:44,  4.06s/it][INFO|2024-11-28 13:11:30] llamafactory.train.callbacks:157 >> {'loss': 1.3328, 'learning_rate': 4.3025e-07, 'epoch': 2.80}\n",
      "{'loss': 1.3328, 'grad_norm': 0.583518385887146, 'learning_rate': 4.302487264785521e-07, 'epoch': 2.8}\n",
      " 97%|███████████████████████████████████████▋ | 180/186 [11:10<00:21,  3.65s/it][INFO|2024-11-28 13:11:48] llamafactory.train.callbacks:157 >> {'loss': 1.2756, 'learning_rate': 1.2827e-07, 'epoch': 2.88}\n",
      "{'loss': 1.2756, 'grad_norm': 0.622009813785553, 'learning_rate': 1.2826691520262114e-07, 'epoch': 2.88}\n",
      " 99%|████████████████████████████████████████▊| 185/186 [11:28<00:03,  3.61s/it][INFO|2024-11-28 13:12:06] llamafactory.train.callbacks:157 >> {'loss': 1.2428, 'learning_rate': 3.5659e-09, 'epoch': 2.96}\n",
      "{'loss': 1.2428, 'grad_norm': 0.6349407434463501, 'learning_rate': 3.565936007254855e-09, 'epoch': 2.96}\n",
      "100%|█████████████████████████████████████████| 186/186 [11:32<00:00,  3.61s/it][INFO|trainer.py:3801] 2024-11-28 13:12:10,378 >> Saving model checkpoint to saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/checkpoint-186\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 13:12:10,400 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 13:12:10,400 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-28 13:12:10,475 >> tokenizer config file saved in saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/checkpoint-186/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-28 13:12:10,475 >> Special tokens file saved in saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/checkpoint-186/special_tokens_map.json\n",
      "[INFO|trainer.py:2584] 2024-11-28 13:12:10,799 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 692.4816, 'train_samples_per_second': 4.332, 'train_steps_per_second': 0.269, 'train_loss': 1.3696343501408894, 'epoch': 2.98}\n",
      "100%|█████████████████████████████████████████| 186/186 [11:32<00:00,  3.72s/it]\n",
      "[INFO|trainer.py:3801] 2024-11-28 13:12:10,801 >> Saving model checkpoint to saves/Qwen2-7B/lora/train_2024-11-28-12-58-29\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 13:12:10,823 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 13:12:10,823 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-28 13:12:10,898 >> tokenizer config file saved in saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-28 13:12:10,898 >> Special tokens file saved in saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      2.976\n",
      "  total_flos               = 21138371GF\n",
      "  train_loss               =     1.3696\n",
      "  train_runtime            = 0:11:32.48\n",
      "  train_samples_per_second =      4.332\n",
      "  train_steps_per_second   =      0.269\n",
      "Figure saved at: saves/Qwen2-7B/lora/train_2024-11-28-12-58-29/training_loss.png\n",
      "[WARNING|2024-11-28 13:12:11] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
      "[WARNING|2024-11-28 13:12:11] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:449] 2024-11-28 13:12:11,164 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 13:48:41,124 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 13:48:41,125 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:42,682 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:42,682 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:42,682 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:42,682 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:42,682 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:42,682 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-28 13:48:45,263 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 13:48:45,263 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 13:48:45,264 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:45,265 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:45,265 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:45,265 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:45,265 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:45,265 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-28 13:48:45,265 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-11-28 13:48:45,501 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:677] 2024-11-28 13:48:45,531 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-28 13:48:45,532 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|2024-11-28 13:48:45] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3934] 2024-11-28 13:48:45,727 >> loading weights file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1670] 2024-11-28 13:48:45,728 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2024-11-28 13:48:45,729 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:56<00:00, 29.17s/it]\n",
      "[INFO|modeling_utils.py:4800] 2024-11-28 13:50:49,991 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-11-28 13:50:49,992 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1049] 2024-11-28 13:50:49,995 >> loading configuration file /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/qwen27/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-11-28 13:50:49,995 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "[INFO|2024-11-28 13:50:50] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-11-28 13:50:51] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
      "[INFO|2024-11-28 13:50:51] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Qwen2-7B/lora/train_2024-11-28-12-58-29\n",
      "[INFO|2024-11-28 13:50:51] llamafactory.model.loader:157 >> all params: 7,615,616,512\n",
      "[WARNING|2024-11-28 13:50:51] llamafactory.chat.hf_engine:168 >> There is no current event loop, creating a new one.\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/workspace/HHXY_PFAI/cs6493nlp/qgevalcap/LLaMA-Factory/\n",
    "!CUDA_VISIBLE_DEVICES=0 USE_MODELSCOPE_HUB=1 python src/webui.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "share": {
   "datetime": "2024-01-26T18:27:52.135Z",
   "image": {
    "name": "modelscope:1.11.0-pytorch2.1.2tensorflow2.14.0-gpu-py310-cu121-ubuntu22.04",
    "url": "dsw-registry-vpc.cn-shanghai.cr.aliyuncs.com/pai/modelscope:1.11.0-pytorch2.1.2tensorflow2.14.0-gpu-py310-cu121-ubuntu22.04"
   },
   "instance": "dsw-7ee0403fc8f6819a",
   "spec": {
    "id": "ecs.gn7i-c8g1.2xlarge",
    "type": "GPU"
   },
   "uid": "1947990434175216"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
